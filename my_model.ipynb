{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3SwAcF5kY8Mn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bweuUp_Y8Mp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Embedding\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "TqzUbbMYY8Mp",
        "outputId": "eff7f4ff-fb66-461e-937e-233d281ffce5"
      },
      "outputs": [],
      "source": [
        "with open(\"training_data.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "    data = data.replace(\"\\n\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vcm8aUCVY8Mq"
      },
      "outputs": [],
      "source": [
        "chars = list(set(list(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edBHO6KXY8Mq"
      },
      "outputs": [],
      "source": [
        "len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXPX3E9GY8Mq"
      },
      "outputs": [],
      "source": [
        "char_to_code = {}\n",
        "code_to_char = {}\n",
        "for char in chars:\n",
        "    char_to_code[char] = len(char_to_code)\n",
        "    code_to_char[len(code_to_char)] = char\n",
        "print(char_to_code)\n",
        "print(code_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7_QvBmsiY8Mq"
      },
      "outputs": [],
      "source": [
        "def encode_string(string):\n",
        "    encoding = []\n",
        "    for char in string:\n",
        "        encoding.append(char_to_code[char])\n",
        "    return encoding\n",
        "\n",
        "def decode_string(string):\n",
        "    decoding = []\n",
        "    for code in string:\n",
        "        decoding.append(code_to_char[code])\n",
        "    return decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N_Z4zmLmY8Mq"
      },
      "outputs": [],
      "source": [
        "input_data = encode_string(data)\n",
        "train = input_data[:int(len(input_data) * 0.9)]\n",
        "test = input_data[int(len(input_data) * 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5BdqMR4xY8Mq"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, keyquery_dim):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wq = self.add_weight(name='query_weights',shape=(self.embed_dim, self.keyquery_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Wk = self.add_weight(name='key_weights',shape=(self.embed_dim, self.keyquery_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Wdown = self.add_weight(name='vdown_weights', shape = (self.embed_dim, self.keyquery_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Wup = self.add_weight(name='vup_weights', shape = (self.keyquery_dim, self.embed_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = tf.matmul(inputs, self.Wq)\n",
        "        k = tf.matmul(inputs, self.Wk)\n",
        "        attention_score = tf.matmul(q, k, transpose_b=True)\n",
        "        attention_score = attention_score / tf.math.sqrt(tf.cast(self.keyquery_dim, tf.float32))\n",
        "        attention_score = tf.linalg.band_part(attention_score, 0, -1) # upper triangular matrix\n",
        "        attention_score = tf.where(tf.equal(attention_score, 0), tf.float32.min, attention_score)\n",
        "        attention_score = tf.nn.softmax(attention_score, axis=-1)\n",
        "\n",
        "        v = tf.matmul(inputs,tf.matmul(self.Wdown, self.Wup))\n",
        "\n",
        "        attention_score = tf.matmul(attention_score, v)\n",
        "\n",
        "        return attention_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4OLuyBCGY8Mr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, num_heads, embed_dim, keyquery_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.attentionheads = []\n",
        "        for i in range(self.num_heads):\n",
        "            self.attentionheads.append(ScaledDotProductAttention(embed_dim=self.embed_dim, keyquery_dim=self.keyquery_dim))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        head_attention_scores = []\n",
        "        for head in self.attentionheads:\n",
        "            head_attention_scores.append(head(inputs))\n",
        "        return tf.math.add_n(head_attention_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mmobAAi4Y8Mr"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(layers.Layer):\n",
        "    def __init__(self, embed_dim, feedforward_dim):\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.feedforward_dim = feedforward_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wup = self.add_weight(name='ffup_weights', shape=(self.embed_dim, self.feedforward_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Bup = self.add_weight(name='ffup_bias', shape=(1, self.feedforward_dim), initializer=tf.zeros_initializer(), trainable=True)\n",
        "        self.Wdown = self.add_weight(name='ffdown_weights', shape=(self.feedforward_dim, self.embed_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Bdown = self.add_weight(name='ffdown_bias', shape=(1, self.embed_dim), initializer=tf.zeros_initializer(), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = tf.matmul(inputs, self.Wup)\n",
        "        x = tf.add(x, self.Bup)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.matmul(x, self.Wdown)\n",
        "        x = tf.add(x, self.Bdown)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bQRVR6TCY8Mr"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, embed_dim, keyquery_dim, feedforward_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "        self.feedforward_dim = feedforward_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.multiheadattention = MultiHeadAttention(self.num_heads, self.embed_dim, self.keyquery_dim)\n",
        "        self.feedforward = MultilayerPerceptron(self.embed_dim, self.feedforward_dim)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mla_output = self.multiheadattention(inputs)\n",
        "        x = self.feedforward(mla_output)+mla_output\n",
        "        x = self.norm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R84hvnsgY8Mr"
      },
      "outputs": [],
      "source": [
        "class Embed(layers.Layer):\n",
        "    def __init__(self, vocab_size,embed_dim, MAXTOKENS):\n",
        "        super(Embed, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.maxtokens = MAXTOKENS\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embed = Embedding(self.vocab_size, self.embed_dim)\n",
        "\n",
        "        self.pos_embed = np.zeros((1,self.maxtokens))\n",
        "\n",
        "        for i in range(self.maxtokens):\n",
        "            if (i%2==0):\n",
        "                self.pos_embed[0][i]=(math.sin(i/(10000**(2*i/self.embed_dim))))\n",
        "            else:\n",
        "                self.pos_embed[0][i]=(math.cos(i/(10000**(2*i/self.embed_dim))))\n",
        "\n",
        "        a = np.array(self.pos_embed)\n",
        "        a = np.expand_dims(a, axis=2)\n",
        "        self.pos_embed = tf.Variable(initial_value=a,trainable=False,dtype=tf.float32)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputshape = tf.shape(inputs)\n",
        "        x = self.embed(inputs)\n",
        "        x = x + self.pos_embed[:,:inputshape[1],:]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5ACdG9s9Y8Mr"
      },
      "outputs": [],
      "source": [
        "class FinalLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, MAXTOKENS):\n",
        "        super(FinalLayer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.MAXTOKENS = MAXTOKENS\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='final_weights', shape=(self.embed_dim, self.MAXTOKENS), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        final_char = inputs[:,-1,:]\n",
        "        x = tf.matmul(final_char, self.W)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7r0qZ5P0Y8Mr"
      },
      "outputs": [],
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(self, num_heads, embed_dim, keyquery_dim, feedforward_dim, MAXTOKENS, num_blocks, vocab_size):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "        self.feedforward_dim = feedforward_dim\n",
        "        self.MAXTOKENS = MAXTOKENS\n",
        "        self.num_blocks = num_blocks\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embed = Embed(self.vocab_size,self.embed_dim, self.MAXTOKENS)\n",
        "        self.transformerblocks = []\n",
        "        for i in range(self.num_blocks):\n",
        "            self.transformerblocks.append(TransformerBlock(self.num_heads, self.embed_dim, self.keyquery_dim, self.feedforward_dim))\n",
        "        self.finallayer = FinalLayer(self.embed_dim, self.MAXTOKENS)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embed(inputs)\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.transformerblocks[i](x)\n",
        "        x = self.finallayer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mRF9kuzxY8Mr"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "token_block_size = 128\n",
        "num_heads = 8\n",
        "vocab_size = len(chars) + 1 # +1 to account for unknown tokens or characters\n",
        "num_heads = 8\n",
        "embed_dim = 512\n",
        "keyquery_dim = 64\n",
        "feedforward_dim = 2048\n",
        "num_blocks = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "p0nyE2_-Y8Ms"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "for i in range(len(train) - token_block_size):\n",
        "    x_train.append(train[i:i+token_block_size])\n",
        "    y_train.append(train[i+token_block_size])\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlh46AlYY8Ms"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNYozVBNY8Ms"
      },
      "outputs": [],
      "source": [
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWbN6wXSY8Ms"
      },
      "outputs": [],
      "source": [
        "print(train[:130])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjYLUz6HY8Ms"
      },
      "outputs": [],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukWjsr24Y8Ms"
      },
      "outputs": [],
      "source": [
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJst93SrY8Ms"
      },
      "outputs": [],
      "source": [
        "model = Transformer(num_heads=num_heads, embed_dim=embed_dim, keyquery_dim=keyquery_dim, feedforward_dim=feedforward_dim, MAXTOKENS=token_block_size, num_blocks=num_blocks, vocab_size=vocab_size)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "model.evaluate(x_train, y_train)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
