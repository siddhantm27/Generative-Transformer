{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3SwAcF5kY8Mn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9bweuUp_Y8Mp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-04 17:51:56.580586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1735993316.649988    9992 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1735993316.668516    9992 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Embedding\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "TqzUbbMYY8Mp",
        "outputId": "eff7f4ff-fb66-461e-937e-233d281ffce5"
      },
      "outputs": [],
      "source": [
        "with open(\"training_data.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "    data = data.replace(\"\\n\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vcm8aUCVY8Mq"
      },
      "outputs": [],
      "source": [
        "chars = list(set(list(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "edBHO6KXY8Mq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mXPX3E9GY8Mq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'c': 0, 'l': 1, '!': 2, 'v': 3, 'Q': 4, 'a': 5, 'y': 6, 'o': 7, 'I': 8, \"'\": 9, 'p': 10, 'B': 11, 'f': 12, 'A': 13, 'm': 14, 'U': 15, 'i': 16, 'F': 17, 'K': 18, 'k': 19, 'b': 20, 'D': 21, 'V': 22, 'r': 23, 'R': 24, 'z': 25, 'e': 26, 'n': 27, 'w': 28, 'E': 29, 'M': 30, 'u': 31, 'd': 32, ',': 33, 'O': 34, 'T': 35, 'x': 36, 'g': 37, 'N': 38, 'P': 39, '&': 40, '3': 41, ';': 42, ':': 43, 'S': 44, 'Y': 45, 'H': 46, 'X': 47, 'h': 48, 'G': 49, ' ': 50, 's': 51, 'q': 52, '.': 53, 'Z': 54, 't': 55, '?': 56, 'J': 57, 'W': 58, 'j': 59, 'L': 60, '-': 61, 'C': 62, '$': 63}\n",
            "{0: 'c', 1: 'l', 2: '!', 3: 'v', 4: 'Q', 5: 'a', 6: 'y', 7: 'o', 8: 'I', 9: \"'\", 10: 'p', 11: 'B', 12: 'f', 13: 'A', 14: 'm', 15: 'U', 16: 'i', 17: 'F', 18: 'K', 19: 'k', 20: 'b', 21: 'D', 22: 'V', 23: 'r', 24: 'R', 25: 'z', 26: 'e', 27: 'n', 28: 'w', 29: 'E', 30: 'M', 31: 'u', 32: 'd', 33: ',', 34: 'O', 35: 'T', 36: 'x', 37: 'g', 38: 'N', 39: 'P', 40: '&', 41: '3', 42: ';', 43: ':', 44: 'S', 45: 'Y', 46: 'H', 47: 'X', 48: 'h', 49: 'G', 50: ' ', 51: 's', 52: 'q', 53: '.', 54: 'Z', 55: 't', 56: '?', 57: 'J', 58: 'W', 59: 'j', 60: 'L', 61: '-', 62: 'C', 63: '$'}\n"
          ]
        }
      ],
      "source": [
        "char_to_code = {}\n",
        "code_to_char = {}\n",
        "for char in chars:\n",
        "    char_to_code[char] = len(char_to_code)\n",
        "    code_to_char[len(code_to_char)] = char\n",
        "print(char_to_code)\n",
        "print(code_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7_QvBmsiY8Mq"
      },
      "outputs": [],
      "source": [
        "def encode_string(string):\n",
        "    encoding = []\n",
        "    for char in string:\n",
        "        encoding.append(char_to_code[char])\n",
        "    return encoding\n",
        "\n",
        "def decode_string(string):\n",
        "    decoding = []\n",
        "    for code in string:\n",
        "        decoding.append(code_to_char[code])\n",
        "    return decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N_Z4zmLmY8Mq"
      },
      "outputs": [],
      "source": [
        "input_data = encode_string(data)\n",
        "train = input_data[:int(len(input_data) * 0.9)]\n",
        "test = input_data[int(len(input_data) * 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5BdqMR4xY8Mq"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, keyquery_dim):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wq = self.add_weight(name='query_weights',shape=(self.embed_dim, self.keyquery_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Wk = self.add_weight(name='key_weights',shape=(self.embed_dim, self.keyquery_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Wdown = self.add_weight(name='vdown_weights', shape = (self.embed_dim, self.keyquery_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Wup = self.add_weight(name='vup_weights', shape = (self.keyquery_dim, self.embed_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = tf.matmul(inputs, self.Wq)\n",
        "        k = tf.matmul(inputs, self.Wk)\n",
        "        attention_score = tf.matmul(q, k, transpose_b=True)\n",
        "        attention_score = attention_score / tf.math.sqrt(tf.cast(self.keyquery_dim, tf.float32))\n",
        "        attention_score = tf.linalg.band_part(attention_score, 0, -1) # upper triangular matrix\n",
        "        attention_score = tf.where(tf.equal(attention_score, 0), tf.float32.min, attention_score)\n",
        "        attention_score = tf.nn.softmax(attention_score, axis=-1)\n",
        "\n",
        "        v = tf.matmul(inputs,tf.matmul(self.Wdown, self.Wup))\n",
        "\n",
        "        attention_score = tf.matmul(attention_score, v)\n",
        "\n",
        "        return attention_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4OLuyBCGY8Mr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, num_heads, embed_dim, keyquery_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.attentionheads = []\n",
        "        for i in range(self.num_heads):\n",
        "            self.attentionheads.append(ScaledDotProductAttention(embed_dim=self.embed_dim, keyquery_dim=self.keyquery_dim))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        head_attention_scores = []\n",
        "        for head in self.attentionheads:\n",
        "            head_attention_scores.append(head(inputs))\n",
        "        return tf.math.add_n(head_attention_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mmobAAi4Y8Mr"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(layers.Layer):\n",
        "    def __init__(self, embed_dim, feedforward_dim):\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.feedforward_dim = feedforward_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wup = self.add_weight(name='ffup_weights', shape=(self.embed_dim, self.feedforward_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Bup = self.add_weight(name='ffup_bias', shape=(1, self.feedforward_dim), initializer=tf.zeros_initializer(), trainable=True)\n",
        "        self.Wdown = self.add_weight(name='ffdown_weights', shape=(self.feedforward_dim, self.embed_dim), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "        self.Bdown = self.add_weight(name='ffdown_bias', shape=(1, self.embed_dim), initializer=tf.zeros_initializer(), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = tf.matmul(inputs, self.Wup)\n",
        "        x = tf.add(x, self.Bup)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.matmul(x, self.Wdown)\n",
        "        x = tf.add(x, self.Bdown)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bQRVR6TCY8Mr"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, embed_dim, keyquery_dim, feedforward_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "        self.feedforward_dim = feedforward_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.multiheadattention = MultiHeadAttention(self.num_heads, self.embed_dim, self.keyquery_dim)\n",
        "        self.feedforward = MultilayerPerceptron(self.embed_dim, self.feedforward_dim)\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mla_output = self.multiheadattention(inputs)\n",
        "        x = self.feedforward(mla_output)+mla_output\n",
        "        x = self.norm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R84hvnsgY8Mr"
      },
      "outputs": [],
      "source": [
        "class Embed(layers.Layer):\n",
        "    def __init__(self, vocab_size,embed_dim, MAXTOKENS):\n",
        "        super(Embed, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.maxtokens = MAXTOKENS\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embed = Embedding(self.vocab_size, self.embed_dim)\n",
        "\n",
        "        self.pos_embed = np.zeros((1,self.maxtokens))\n",
        "\n",
        "        for i in range(self.maxtokens):\n",
        "            if (i%2==0):\n",
        "                self.pos_embed[0][i]=(math.sin(i/(10000**(2*i/self.embed_dim))))\n",
        "            else:\n",
        "                self.pos_embed[0][i]=(math.cos(i/(10000**(2*i/self.embed_dim))))\n",
        "\n",
        "        a = np.array(self.pos_embed)\n",
        "        a = np.expand_dims(a, axis=2)\n",
        "        self.pos_embed = tf.Variable(initial_value=a,trainable=False,dtype=tf.float32)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputshape = tf.shape(inputs)\n",
        "        x = self.embed(inputs)\n",
        "        x = x + self.pos_embed[:,:inputshape[1],:]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5ACdG9s9Y8Mr"
      },
      "outputs": [],
      "source": [
        "class FinalLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, MAXTOKENS):\n",
        "        super(FinalLayer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.MAXTOKENS = MAXTOKENS\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='final_weights', shape=(self.embed_dim, self.MAXTOKENS), initializer=tf.random_normal_initializer(), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        final_char = inputs[:,-1,:]\n",
        "        x = tf.matmul(final_char, self.W)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7r0qZ5P0Y8Mr"
      },
      "outputs": [],
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(self, num_heads, embed_dim, keyquery_dim, feedforward_dim, MAXTOKENS, num_blocks, vocab_size):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.keyquery_dim = keyquery_dim\n",
        "        self.feedforward_dim = feedforward_dim\n",
        "        self.MAXTOKENS = MAXTOKENS\n",
        "        self.num_blocks = num_blocks\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embed = Embed(self.vocab_size,self.embed_dim, self.MAXTOKENS)\n",
        "        self.transformerblocks = []\n",
        "        for i in range(self.num_blocks):\n",
        "            self.transformerblocks.append(TransformerBlock(self.num_heads, self.embed_dim, self.keyquery_dim, self.feedforward_dim))\n",
        "        self.finallayer = FinalLayer(self.embed_dim, self.MAXTOKENS)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embed(inputs)\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.transformerblocks[i](x)\n",
        "        x = self.finallayer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mRF9kuzxY8Mr"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "token_block_size = 128\n",
        "num_heads = 8\n",
        "vocab_size = len(chars) + 1 # +1 to account for unknown tokens or characters\n",
        "num_heads = 8\n",
        "embed_dim = 512\n",
        "keyquery_dim = 64\n",
        "feedforward_dim = 2048\n",
        "num_blocks = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "p0nyE2_-Y8Ms"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "for i in range(len(train) - token_block_size):\n",
        "    x_train.append(train[i:i+token_block_size])\n",
        "    y_train.append(train[i+token_block_size])\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vlh46AlYY8Ms"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1003726, 128)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NNYozVBNY8Ms"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1003726,)\n"
          ]
        }
      ],
      "source": [
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bWbN6wXSY8Ms"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[17, 16, 23, 51, 55, 50, 62, 16, 55, 16, 25, 26, 27, 43, 50, 11, 26, 12, 7, 23, 26, 50, 28, 26, 50, 10, 23, 7, 0, 26, 26, 32, 50, 5, 27, 6, 50, 12, 31, 23, 55, 48, 26, 23, 33, 50, 48, 26, 5, 23, 50, 14, 26, 50, 51, 10, 26, 5, 19, 53, 50, 50, 13, 1, 1, 43, 50, 44, 10, 26, 5, 19, 33, 50, 51, 10, 26, 5, 19, 53, 50, 50, 17, 16, 23, 51, 55, 50, 62, 16, 55, 16, 25, 26, 27, 43, 50, 45, 7, 31, 50, 5, 23, 26, 50, 5, 1, 1, 50, 23, 26, 51, 7, 1, 3, 26, 32, 50, 23, 5, 55, 48, 26, 23, 50, 55, 7, 50, 32, 16]\n"
          ]
        }
      ],
      "source": [
        "print(train[:130])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bjYLUz6HY8Ms"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[17 16 23 51 55 50 62 16 55 16 25 26 27 43 50 11 26 12  7 23 26 50 28 26\n",
            " 50 10 23  7  0 26 26 32 50  5 27  6 50 12 31 23 55 48 26 23 33 50 48 26\n",
            "  5 23 50 14 26 50 51 10 26  5 19 53 50 50 13  1  1 43 50 44 10 26  5 19\n",
            " 33 50 51 10 26  5 19 53 50 50 17 16 23 51 55 50 62 16 55 16 25 26 27 43\n",
            " 50 45  7 31 50  5 23 26 50  5  1  1 50 23 26 51  7  1  3 26 32 50 23  5\n",
            " 55 48 26 23 50 55  7 50]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ukWjsr24Y8Ms"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JJst93SrY8Ms"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1735993374.779192    9992 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2278 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2050, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1735993403.302667   10573 service.cc:148] XLA service 0x779af002d5f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1735993403.302708   10573 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 2050, Compute Capability 8.6\n",
            "I0000 00:00:1735993407.383905   10573 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "I0000 00:00:1735993423.474398   10573 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "ename": "ResourceExhaustedError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_9992/3547343586.py\", line 7, in <module>\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1394586936 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_53804]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(num_heads\u001b[38;5;241m=\u001b[39mnum_heads, embed_dim\u001b[38;5;241m=\u001b[39membed_dim, keyquery_dim\u001b[38;5;241m=\u001b[39mkeyquery_dim, feedforward_dim\u001b[38;5;241m=\u001b[39mfeedforward_dim, MAXTOKENS\u001b[38;5;241m=\u001b[39mtoken_block_size, num_blocks\u001b[38;5;241m=\u001b[39mnum_blocks, vocab_size\u001b[38;5;241m=\u001b[39mvocab_size)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m      5\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(x_train, y_train)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_9992/3547343586.py\", line 7, in <module>\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/home/siddhant/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1394586936 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_53804]"
          ]
        }
      ],
      "source": [
        "model = Transformer(num_heads=num_heads, embed_dim=embed_dim, keyquery_dim=keyquery_dim, feedforward_dim=feedforward_dim, MAXTOKENS=token_block_size, num_blocks=num_blocks, vocab_size=vocab_size)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "model.evaluate(x_train, y_train)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
